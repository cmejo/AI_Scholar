#!/usr/bin/env python3
"""
Ubuntu Compatibility Test Runner

This script runs the Ubuntu compatibility testing framework and generates
comprehensive reports for deployment readiness assessment.
"""

import os
import sys
import json
import argparse
import logging
from datetime import datetime
from pathlib import Path

# Add the scripts directory to the path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from ubuntu_compatibility_tester import UbuntuCompatibilityTestFramework, TestResult

def setup_logging(verbose: bool = False):
    """Set up logging configuration"""
    level = logging.DEBUG if verbose else logging.INFO
    logging.basicConfig(
        level=level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(sys.stdout),
            logging.FileHandler('ubuntu_compatibility_test.log')
        ]
    )

def generate_html_report(report_data: dict, output_file: str):
    """Generate an HTML report from test results"""
    html_template = """
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ubuntu Compatibility Test Report</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }
        .container { max-width: 1200px; margin: 0 auto; background: white; padding: 20px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }
        .header { text-align: center; margin-bottom: 30px; }
        .summary { display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 20px; margin-bottom: 30px; }
        .summary-card { background: #f8f9fa; padding: 15px; border-radius: 6px; text-align: center; }
        .summary-card h3 { margin: 0 0 10px 0; color: #333; }
        .summary-card .value { font-size: 24px; font-weight: bold; }
        .status-excellent { color: #28a745; }
        .status-good { color: #17a2b8; }
        .status-needs-attention { color: #ffc107; }
        .status-critical { color: #dc3545; }
        .test-results { margin-bottom: 30px; }
        .test-result { margin-bottom: 15px; padding: 15px; border-radius: 6px; border-left: 4px solid; }
        .test-pass { background: #d4edda; border-color: #28a745; }
        .test-warning { background: #fff3cd; border-color: #ffc107; }
        .test-fail { background: #f8d7da; border-color: #dc3545; }
        .test-skip { background: #e2e3e5; border-color: #6c757d; }
        .test-name { font-weight: bold; margin-bottom: 5px; }
        .test-message { margin-bottom: 10px; }
        .test-details { font-size: 12px; color: #666; }
        .recommendations { background: #e7f3ff; padding: 20px; border-radius: 6px; }
        .recommendations ul { margin: 10px 0; padding-left: 20px; }
        .footer { text-align: center; margin-top: 30px; color: #666; font-size: 12px; }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>Ubuntu Compatibility Test Report</h1>
            <p>Ubuntu Version: {ubuntu_version} | Generated: {timestamp}</p>
        </div>
        
        <div class="summary">
            <div class="summary-card">
                <h3>Overall Status</h3>
                <div class="value status-{status_class}">{overall_status}</div>
            </div>
            <div class="summary-card">
                <h3>Compatibility Score</h3>
                <div class="value">{score}%</div>
            </div>
            <div class="summary-card">
                <h3>Tests Passed</h3>
                <div class="value" style="color: #28a745;">{passed}</div>
            </div>
            <div class="summary-card">
                <h3>Warnings</h3>
                <div class="value" style="color: #ffc107;">{warnings}</div>
            </div>
            <div class="summary-card">
                <h3>Failed</h3>
                <div class="value" style="color: #dc3545;">{failed}</div>
            </div>
            <div class="summary-card">
                <h3>Skipped</h3>
                <div class="value" style="color: #6c757d;">{skipped}</div>
            </div>
        </div>
        
        <div class="test-results">
            <h2>Test Results</h2>
            {test_results_html}
        </div>
        
        <div class="recommendations">
            <h2>Recommendations</h2>
            <ul>
                {recommendations_html}
            </ul>
        </div>
        
        <div class="footer">
            <p>Generated by Ubuntu Compatibility Testing Framework</p>
        </div>
    </div>
</body>
</html>
    """
    
    # Prepare data for template
    summary = report_data["test_summary"]
    status_class = summary["overall_status"].lower().replace("_", "-")
    
    # Generate test results HTML
    test_results_html = ""
    for result in report_data["test_results"]:
        result_class = f"test-{result['result'].lower()}"
        test_results_html += f"""
        <div class="test-result {result_class}">
            <div class="test-name">{result['test_name'].replace('_', ' ').title()}</div>
            <div class="test-message">{result['message']}</div>
            <div class="test-details">
                Execution time: {result['execution_time']:.2f}s | 
                Ubuntu specific: {'Yes' if result['ubuntu_specific'] else 'No'}
            </div>
        </div>
        """
    
    # Generate recommendations HTML
    recommendations_html = ""
    for rec in report_data["recommendations"]:
        recommendations_html += f"<li>{rec}</li>"
    
    # Fill template
    html_content = html_template.format(
        ubuntu_version=report_data["ubuntu_version"],
        timestamp=datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        status_class=status_class,
        overall_status=summary["overall_status"],
        score=summary["score"],
        passed=summary["passed"],
        warnings=summary["warnings"],
        failed=summary["failed"],
        skipped=summary["skipped"],
        test_results_html=test_results_html,
        recommendations_html=recommendations_html
    )
    
    with open(output_file, 'w') as f:
        f.write(html_content)

def generate_markdown_report(report_data: dict, output_file: str):
    """Generate a Markdown report from test results"""
    summary = report_data["test_summary"]
    
    markdown_content = f"""# Ubuntu Compatibility Test Report

**Ubuntu Version:** {report_data["ubuntu_version"]}  
**Generated:** {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

## Summary

| Metric | Value |
|--------|-------|
| Overall Status | **{summary["overall_status"]}** |
| Compatibility Score | **{summary["score"]}%** |
| Tests Passed | {summary["passed"]} |
| Warnings | {summary["warnings"]} |
| Failed | {summary["failed"]} |
| Skipped | {summary["skipped"]} |
| Total Tests | {summary["total_tests"]} |

## Test Results

"""
    
    for result in report_data["test_results"]:
        status_emoji = {
            "PASS": "✅",
            "WARNING": "⚠️",
            "FAIL": "❌",
            "SKIP": "⏭️"
        }.get(result["result"], "❓")
        
        markdown_content += f"""### {status_emoji} {result['test_name'].replace('_', ' ').title()}

**Status:** {result['result']}  
**Message:** {result['message']}  
**Execution Time:** {result['execution_time']:.2f}s  
**Ubuntu Specific:** {'Yes' if result['ubuntu_specific'] else 'No'}

"""
        
        if result.get('details') and isinstance(result['details'], dict):
            if 'issues' in result['details'] and result['details']['issues']:
                markdown_content += "**Issues Found:**\n"
                for issue in result['details']['issues']:
                    markdown_content += f"- {issue}\n"
                markdown_content += "\n"
    
    markdown_content += """## Recommendations

"""
    
    for rec in report_data["recommendations"]:
        markdown_content += f"- {rec}\n"
    
    if report_data["ubuntu_specific_issues"]:
        markdown_content += """
## Ubuntu-Specific Issues

"""
        for issue in report_data["ubuntu_specific_issues"]:
            markdown_content += f"""### {issue['test_name'].replace('_', ' ').title()}
- **Status:** {issue['result']}
- **Message:** {issue['message']}

"""
    
    markdown_content += """
---
*Generated by Ubuntu Compatibility Testing Framework*
"""
    
    with open(output_file, 'w') as f:
        f.write(markdown_content)

def main():
    """Main function"""
    parser = argparse.ArgumentParser(
        description="Run Ubuntu compatibility tests for AI Scholar application"
    )
    parser.add_argument(
        "--ubuntu-version", 
        default="24.04", 
        help="Ubuntu version to test against (default: 24.04)"
    )
    parser.add_argument(
        "--output-dir", 
        default="ubuntu-compatibility-reports", 
        help="Output directory for reports (default: ubuntu-compatibility-reports)"
    )
    parser.add_argument(
        "--format", 
        choices=["json", "html", "markdown", "all"], 
        default="all",
        help="Output format (default: all)"
    )
    parser.add_argument(
        "--verbose", "-v", 
        action="store_true", 
        help="Enable verbose logging"
    )
    parser.add_argument(
        "--quick", 
        action="store_true", 
        help="Run quick tests only (skip Docker and performance tests)"
    )
    
    args = parser.parse_args()
    
    # Set up logging
    setup_logging(args.verbose)
    logger = logging.getLogger(__name__)
    
    # Create output directory
    output_dir = Path(args.output_dir)
    output_dir.mkdir(exist_ok=True)
    
    logger.info(f"Starting Ubuntu compatibility tests for version {args.ubuntu_version}")
    logger.info(f"Output directory: {output_dir}")
    
    try:
        # Initialize test framework
        framework = UbuntuCompatibilityTestFramework(args.ubuntu_version)
        
        if args.quick:
            logger.info("Running quick tests (file system and network only)")
            # Run only system integration tests
            results = []
            results.append(framework.system_tester.test_file_system_permissions())
            results.append(framework.system_tester.test_network_configuration())
            framework.results = results
        else:
            logger.info("Running full test suite")
            # Run all tests
            results = framework.run_all_tests()
        
        # Generate report
        logger.info("Generating test report")
        report = framework.generate_report()
        
        # Create timestamp for file names
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Save reports in requested formats
        if args.format in ["json", "all"]:
            json_file = output_dir / f"ubuntu_compatibility_report_{timestamp}.json"
            with open(json_file, 'w') as f:
                json.dump(report, f, indent=2)
            logger.info(f"JSON report saved to: {json_file}")
        
        if args.format in ["html", "all"]:
            html_file = output_dir / f"ubuntu_compatibility_report_{timestamp}.html"
            generate_html_report(report, html_file)
            logger.info(f"HTML report saved to: {html_file}")
        
        if args.format in ["markdown", "all"]:
            md_file = output_dir / f"ubuntu_compatibility_report_{timestamp}.md"
            generate_markdown_report(report, md_file)
            logger.info(f"Markdown report saved to: {md_file}")
        
        # Print summary to console
        summary = report["test_summary"]
        print("\n" + "="*60)
        print("UBUNTU COMPATIBILITY TEST SUMMARY")
        print("="*60)
        print(f"Ubuntu Version: {args.ubuntu_version}")
        print(f"Overall Status: {summary['overall_status']}")
        print(f"Compatibility Score: {summary['score']}%")
        print(f"Tests: {summary['passed']} passed, {summary['warnings']} warnings, {summary['failed']} failed, {summary['skipped']} skipped")
        
        if report["recommendations"]:
            print("\nKey Recommendations:")
            for i, rec in enumerate(report["recommendations"][:3], 1):
                print(f"{i}. {rec}")
        
        if report["ubuntu_specific_issues"]:
            print(f"\nUbuntu-specific issues found: {len(report['ubuntu_specific_issues'])}")
        
        print(f"\nDetailed reports saved to: {output_dir}")
        print("="*60)
        
        # Exit with appropriate code
        if summary["failed"] > 0:
            logger.error("Tests failed - check reports for details")
            return 1
        elif summary["warnings"] > 0:
            logger.warning("Tests completed with warnings - review recommended")
            return 2
        else:
            logger.info("All tests passed successfully")
            return 0
            
    except Exception as e:
        logger.error(f"Test execution failed: {str(e)}")
        if args.verbose:
            import traceback
            logger.error(traceback.format_exc())
        return 1

if __name__ == "__main__":
    sys.exit(main())