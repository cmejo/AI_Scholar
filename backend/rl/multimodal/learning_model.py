"""
Multi-modal learning model for training and inference on integrated features.
"""

import asyncio
import logging
from typing import Dict, List, Optional, Any, Tuple, Union
import numpy as np
from dataclasses import dataclass, field
from enum import Enum
import json
from datetime import datetime
import pickle
from pathlib import Path

from .models import MultiModalFeatures, MultiModalContext
from .feature_integrator import IntegrationConfig

logger = logging.getLogger(__name__)


class ModelType(Enum):
    """Types of multi-modal learning models."""
    NEURAL_NETWORK = "neural_network"
    TRANSFORMER = "transformer"
    ENSEMBLE = "ensemble"
    REINFORCEMENT_LEARNING = "reinforcement_learning"


class TrainingMode(Enum):
    """Training modes for the model."""
    SUPERVISED = "supervised"
    UNSUPERVISED = "unsupervised"
    SEMI_SUPERVISED = "semi_supervised"
    REINFORCEMENT = "reinforcement"
    SELF_SUPERVISED = "self_supervised"


@dataclass
class ModelConfig:
    """Configuration for multi-modal learning model."""
    model_type: ModelType = ModelType.NEURAL_NETWORK
    training_mode: TrainingMode = TrainingMode.SUPERVISED
    
    # Architecture parameters
    hidden_layers: List[int] = field(default_factory=lambda: [512, 256, 128])
    activation_function: str = "relu"
    dropout_rate: float = 0.2
    batch_normalization: bool = True
    
    # Training parameters
    learning_rate: float = 0.001
    batch_size: int = 32
    max_epochs: int = 100
    early_stopping_patience: int = 10
    validation_split: float = 0.2
    
    # Regularization
    l1_regularization: float = 0.0
    l2_regularization: float = 0.01
    gradient_clipping: float = 1.0
    
    # Model saving
    save_best_only: bool = True
    save_frequency: int = 10
    model_save_path: str = "models/multimodal/"
    
    # Evaluation
    evaluation_metrics: List[str] = field(default_factory=lambda: ["accuracy", "precision", "recall", "f1"])
    
    # Multi-modal specific
    feature_fusion_strategy: str = "late_fusion"
    cross_modal_attention: bool = True
    modality_dropout: float = 0.1


@dataclass
class MultiModalTrainingExample:
    """Training example for multi-modal learning."""
    features: MultiModalFeatures
    target: Union[float, int, np.ndarray, Dict[str, Any]]
    context: Optional[MultiModalContext] = None
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def __post_init__(self):
        """Validate training example."""
        if self.features is None:
            raise ValueError("Features cannot be None")
        if self.target is None:
            raise ValueError("Target cannot be None")


@dataclass
class TrainingResults:
    """Results from model training."""
    training_history: Dict[str, List[float]]
    final_metrics: Dict[str, float]
    best_epoch: int
    total_epochs: int
    training_time: float
    model_path: Optional[str] = None
    convergence_info: Dict[str, Any] = field(default_factory=dict)


@dataclass
class Recommendation:
    """Recommendation generated by the model."""
    recommendation_id: str
    content: str
    confidence: float
    reasoning: str
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def __post_init__(self):
        """Validate recommendation."""
        if not 0 <= self.confidence <= 1:
            raise ValueError("Confidence must be between 0 and 1")


class ActivationFunction:
    """Activation functions for neural networks."""
    
    @staticmethod
    def relu(x: np.ndarray) -> np.ndarray:
        """ReLU activation function."""
        return np.maximum(0, x)
    
    @staticmethod
    def sigmoid(x: np.ndarray) -> np.ndarray:
        """Sigmoid activation function."""
        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))
    
    @staticmethod
    def tanh(x: np.ndarray) -> np.ndarray:
        """Tanh activation function."""
        return np.tanh(x)
    
    @staticmethod
    def softmax(x: np.ndarray) -> np.ndarray:
        """Softmax activation function."""
        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)
    
    @staticmethod
    def get_activation(name: str):
        """Get activation function by name."""
        activations = {
            "relu": ActivationFunction.relu,
            "sigmoid": ActivationFunction.sigmoid,
            "tanh": ActivationFunction.tanh,
            "softmax": ActivationFunction.softmax
        }
        return activations.get(name.lower(), ActivationFunction.relu)


class NeuralLayer:
    """Simple neural network layer."""
    
    def __init__(self, input_size: int, output_size: int, activation: str = "relu", dropout_rate: float = 0.0):
        self.input_size = input_size
        self.output_size = output_size
        self.activation_name = activation
        self.activation_fn = ActivationFunction.get_activation(activation)
        self.dropout_rate = dropout_rate
        
        # Initialize weights and biases
        self.weights = np.random.randn(input_size, output_size) * np.sqrt(2.0 / input_size)
        self.biases = np.zeros(output_size)
        
        # For training
        self.last_input = None
        self.last_output = None
    
    def forward(self, x: np.ndarray, training: bool = False) -> np.ndarray:
        """Forward pass through the layer."""
        self.last_input = x
        
        # Linear transformation
        linear_output = np.dot(x, self.weights) + self.biases
        
        # Activation
        activated_output = self.activation_fn(linear_output)
        
        # Dropout (only during training)
        if training and self.dropout_rate > 0:
            dropout_mask = np.random.binomial(1, 1 - self.dropout_rate, activated_output.shape)
            activated_output = activated_output * dropout_mask / (1 - self.dropout_rate)
        
        self.last_output = activated_output
        return activated_output
    
    def backward(self, grad_output: np.ndarray, learning_rate: float = 0.001) -> np.ndarray:
        """Backward pass through the layer (simplified)."""
        # This is a simplified implementation
        # In practice, you'd use proper gradient computation
        
        # Gradient w.r.t. weights
        grad_weights = np.dot(self.last_input.T, grad_output)
        
        # Gradient w.r.t. biases
        grad_biases = np.sum(grad_output, axis=0)
        
        # Gradient w.r.t. input
        grad_input = np.dot(grad_output, self.weights.T)
        
        # Update weights and biases
        self.weights -= learning_rate * grad_weights
        self.biases -= learning_rate * grad_biases
        
        return grad_input


class MultiModalNeuralNetwork:
    """Simple multi-modal neural network implementation."""
    
    def __init__(self, config: ModelConfig):
        self.config = config
        self.layers = []
        self.is_trained = False
        self.training_history = {"loss": [], "accuracy": []}
    
    def build_model(self, input_size: int, output_size: int):
        """Build the neural network architecture."""
        self.layers = []
        
        # Input layer
        current_size = input_size
        
        # Hidden layers
        for hidden_size in self.config.hidden_layers:
            layer = NeuralLayer(
                current_size, 
                hidden_size, 
                self.config.activation_function,
                self.config.dropout_rate
            )
            self.layers.append(layer)
            current_size = hidden_size
        
        # Output layer
        output_layer = NeuralLayer(current_size, output_size, "sigmoid", 0.0)
        self.layers.append(output_layer)
    
    async def train(self, training_data: List[MultiModalTrainingExample]) -> TrainingResults:
        """Train the neural network."""
        start_time = datetime.now()
        
        if not training_data:
            raise ValueError("Training data cannot be empty")
        
        # Prepare data
        X, y = await self._prepare_training_data(training_data)
        
        # Build model if not already built
        if not self.layers:
            input_size = X.shape[1]
            output_size = y.shape[1] if len(y.shape) > 1 else 1
            self.build_model(input_size, output_size)
        
        # Training loop
        best_loss = float('inf')
        patience_counter = 0
        
        for epoch in range(self.config.max_epochs):
            # Forward pass
            predictions = await self._forward_pass(X, training=True)
            
            # Calculate loss
            loss = await self._calculate_loss(predictions, y)
            
            # Backward pass (simplified)
            await self._backward_pass(predictions, y)
            
            # Calculate metrics
            accuracy = await self._calculate_accuracy(predictions, y)
            
            # Store history
            self.training_history["loss"].append(loss)
            self.training_history["accuracy"].append(accuracy)
            
            # Early stopping
            if loss < best_loss:
                best_loss = loss
                patience_counter = 0
                best_epoch = epoch
            else:
                patience_counter += 1
                if patience_counter >= self.config.early_stopping_patience:
                    logger.info(f"Early stopping at epoch {epoch}")
                    break
            
            if epoch % 10 == 0:
                logger.info(f"Epoch {epoch}: Loss = {loss:.4f}, Accuracy = {accuracy:.4f}")
        
        self.is_trained = True
        training_time = (datetime.now() - start_time).total_seconds()
        
        # Calculate final metrics
        final_predictions = await self._forward_pass(X, training=False)
        final_metrics = await self._calculate_all_metrics(final_predictions, y)
        
        return TrainingResults(
            training_history=self.training_history,
            final_metrics=final_metrics,
            best_epoch=best_epoch,
            total_epochs=epoch + 1,
            training_time=training_time,
            convergence_info={"best_loss": best_loss, "final_loss": loss}
        )
    
    async def predict(self, features: MultiModalFeatures) -> np.ndarray:
        """Make predictions using the trained model."""
        if not self.is_trained:
            raise ValueError("Model must be trained before making predictions")
        
        # Convert features to input vector
        input_vector = await self._features_to_vector(features)
        input_vector = input_vector.reshape(1, -1)  # Add batch dimension
        
        # Forward pass
        prediction = await self._forward_pass(input_vector, training=False)
        
        return prediction[0]  # Remove batch dimension
    
    async def _prepare_training_data(self, training_data: List[MultiModalTrainingExample]) -> Tuple[np.ndarray, np.ndarray]:
        """Prepare training data for the model."""
        X = []
        y = []
        
        for example in training_data:
            # Convert features to vector
            feature_vector = await self._features_to_vector(example.features)
            X.append(feature_vector)
            
            # Convert target to vector
            target_vector = await self._target_to_vector(example.target)
            y.append(target_vector)
        
        return np.array(X), np.array(y)
    
    async def _features_to_vector(self, features: MultiModalFeatures) -> np.ndarray:
        """Convert MultiModalFeatures to input vector."""
        # Use the integrated embedding as the main feature vector
        feature_vector = features.integrated_embedding.copy()
        
        # Add confidence scores as additional features
        confidence_values = list(features.confidence_scores.values())
        
        # Combine all features
        combined_features = np.concatenate([feature_vector, confidence_values])
        
        return combined_features
    
    async def _target_to_vector(self, target: Union[float, int, np.ndarray, Dict[str, Any]]) -> np.ndarray:
        """Convert target to vector format."""
        if isinstance(target, (int, float)):
            return np.array([target])
        elif isinstance(target, np.ndarray):
            return target.flatten()
        elif isinstance(target, dict):
            # Convert dictionary to vector (sorted by keys for consistency)
            sorted_keys = sorted(target.keys())
            return np.array([target[key] for key in sorted_keys])
        else:
            raise ValueError(f"Unsupported target type: {type(target)}")
    
    async def _forward_pass(self, x: np.ndarray, training: bool = False) -> np.ndarray:
        """Forward pass through the network."""
        current_input = x
        
        for layer in self.layers:
            current_input = layer.forward(current_input, training=training)
        
        return current_input
    
    async def _backward_pass(self, predictions: np.ndarray, targets: np.ndarray):
        """Backward pass through the network (simplified)."""
        # Calculate output gradient
        grad_output = predictions - targets
        
        # Backpropagate through layers
        current_grad = grad_output
        for layer in reversed(self.layers):
            current_grad = layer.backward(current_grad, self.config.learning_rate)
    
    async def _calculate_loss(self, predictions: np.ndarray, targets: np.ndarray) -> float:
        """Calculate loss (mean squared error)."""
        return np.mean((predictions - targets) ** 2)
    
    async def _calculate_accuracy(self, predictions: np.ndarray, targets: np.ndarray) -> float:
        """Calculate accuracy (simplified for regression)."""
        # For regression, use R-squared as accuracy measure
        ss_res = np.sum((targets - predictions) ** 2)
        ss_tot = np.sum((targets - np.mean(targets)) ** 2)
        
        if ss_tot == 0:
            return 1.0
        
        r_squared = 1 - (ss_res / ss_tot)
        return max(0.0, r_squared)  # Ensure non-negative
    
    async def _calculate_all_metrics(self, predictions: np.ndarray, targets: np.ndarray) -> Dict[str, float]:
        """Calculate all evaluation metrics."""
        metrics = {}
        
        # Basic metrics
        metrics["mse"] = float(np.mean((predictions - targets) ** 2))
        metrics["mae"] = float(np.mean(np.abs(predictions - targets)))
        metrics["rmse"] = float(np.sqrt(metrics["mse"]))
        
        # R-squared
        ss_res = np.sum((targets - predictions) ** 2)
        ss_tot = np.sum((targets - np.mean(targets)) ** 2)
        metrics["r_squared"] = float(1 - (ss_res / ss_tot)) if ss_tot != 0 else 1.0
        
        # Correlation
        if predictions.size > 1 and targets.size > 1:
            correlation_matrix = np.corrcoef(predictions.flatten(), targets.flatten())
            metrics["correlation"] = float(correlation_matrix[0, 1]) if not np.isnan(correlation_matrix[0, 1]) else 0.0
        else:
            metrics["correlation"] = 0.0
        
        return metrics


class RecommendationEngine:
    """Engine for generating recommendations based on multi-modal features."""
    
    def __init__(self, model: MultiModalNeuralNetwork):
        self.model = model
        self.recommendation_templates = {
            "high_confidence": "Based on the analysis, I recommend: {content} (Confidence: {confidence:.2f})",
            "medium_confidence": "You might consider: {content} (Confidence: {confidence:.2f})",
            "low_confidence": "One possibility is: {content} (Confidence: {confidence:.2f})"
        }
    
    async def generate_recommendations(
        self, 
        context: MultiModalContext, 
        num_recommendations: int = 3
    ) -> List[Recommendation]:
        """Generate recommendations based on multi-modal context."""
        
        recommendations = []
        
        # This is a simplified implementation
        # In practice, you'd have more sophisticated recommendation logic
        
        for i in range(num_recommendations):
            # Generate mock recommendation
            confidence = np.random.uniform(0.3, 0.9)
            
            # Select template based on confidence
            if confidence > 0.7:
                template_key = "high_confidence"
            elif confidence > 0.5:
                template_key = "medium_confidence"
            else:
                template_key = "low_confidence"
            
            content = f"Recommendation {i+1} based on document analysis"
            reasoning = f"Generated based on {len(context.visual_elements)} visual elements and text analysis"
            
            recommendation = Recommendation(
                recommendation_id=f"rec_{i+1}_{datetime.now().timestamp()}",
                content=content,
                confidence=confidence,
                reasoning=reasoning,
                metadata={
                    "generation_method": "neural_network",
                    "context_id": context.context_id,
                    "timestamp": datetime.now().isoformat()
                }
            )
            
            recommendations.append(recommendation)
        
        # Sort by confidence
        recommendations.sort(key=lambda x: x.confidence, reverse=True)
        
        return recommendations


class ModelPersistence:
    """Handles saving and loading of trained models."""
    
    @staticmethod
    async def save_model(model: MultiModalNeuralNetwork, filepath: str) -> bool:
        """Save model to file."""
        try:
            model_data = {
                "config": model.config,
                "layers": [],
                "is_trained": model.is_trained,
                "training_history": model.training_history
            }
            
            # Save layer parameters
            for layer in model.layers:
                layer_data = {
                    "input_size": layer.input_size,
                    "output_size": layer.output_size,
                    "activation_name": layer.activation_name,
                    "dropout_rate": layer.dropout_rate,
                    "weights": layer.weights.tolist(),
                    "biases": layer.biases.tolist()
                }
                model_data["layers"].append(layer_data)
            
            # Ensure directory exists
            Path(filepath).parent.mkdir(parents=True, exist_ok=True)
            
            # Save to file
            with open(filepath, 'wb') as f:
                pickle.dump(model_data, f)
            
            logger.info(f"Model saved to {filepath}")
            return True
            
        except Exception as e:
            logger.error(f"Error saving model: {str(e)}")
            return False
    
    @staticmethod
    async def load_model(filepath: str) -> Optional[MultiModalNeuralNetwork]:
        """Load model from file."""
        try:
            with open(filepath, 'rb') as f:
                model_data = pickle.load(f)
            
            # Recreate model
            model = MultiModalNeuralNetwork(model_data["config"])
            model.is_trained = model_data["is_trained"]
            model.training_history = model_data["training_history"]
            
            # Recreate layers
            model.layers = []
            for layer_data in model_data["layers"]:
                layer = NeuralLayer(
                    layer_data["input_size"],
                    layer_data["output_size"],
                    layer_data["activation_name"],
                    layer_data["dropout_rate"]
                )
                layer.weights = np.array(layer_data["weights"])
                layer.biases = np.array(layer_data["biases"])
                model.layers.append(layer)
            
            logger.info(f"Model loaded from {filepath}")
            return model
            
        except Exception as e:
            logger.error(f"Error loading model: {str(e)}")
            return None


class MultiModalLearningModel:
    """Main multi-modal learning model that coordinates training and inference."""
    
    def __init__(self, config: Optional[ModelConfig] = None):
        self.config = config or ModelConfig()
        self.model = MultiModalNeuralNetwork(self.config)
        self.recommendation_engine = RecommendationEngine(self.model)
        self.is_trained = False
    
    async def train_on_multimodal_data(
        self, 
        training_data: List[MultiModalTrainingExample]
    ) -> TrainingResults:
        """Train the model on multi-modal data."""
        
        if not training_data:
            raise ValueError("Training data cannot be empty")
        
        logger.info(f"Starting training with {len(training_data)} examples")
        
        # Train the neural network
        results = await self.model.train(training_data)
        
        self.is_trained = True
        
        # Save model if configured
        if self.config.save_best_only:
            model_path = f"{self.config.model_save_path}/best_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl"
            success = await ModelPersistence.save_model(self.model, model_path)
            if success:
                results.model_path = model_path
        
        logger.info(f"Training completed. Final metrics: {results.final_metrics}")
        
        return results
    
    async def generate_multimodal_recommendations(
        self, 
        context: MultiModalContext
    ) -> List[Recommendation]:
        """Generate recommendations based on multi-modal context."""
        
        if not self.is_trained:
            logger.warning("Model not trained, generating basic recommendations")
        
        recommendations = await self.recommendation_engine.generate_recommendations(context)
        
        logger.info(f"Generated {len(recommendations)} recommendations")
        
        return recommendations
    
    async def predict_from_features(self, features: MultiModalFeatures) -> Dict[str, Any]:
        """Make predictions from multi-modal features."""
        
        if not self.is_trained:
            raise ValueError("Model must be trained before making predictions")
        
        # Get model prediction
        prediction = await self.model.predict(features)
        
        # Calculate confidence based on prediction certainty
        confidence = float(np.max(prediction)) if len(prediction) > 1 else float(prediction[0])
        
        return {
            "prediction": prediction.tolist(),
            "confidence": confidence,
            "feature_quality": features.get_overall_confidence(),
            "metadata": {
                "model_type": self.config.model_type.value,
                "prediction_timestamp": datetime.now().isoformat()
            }
        }
    
    async def evaluate_model(self, test_data: List[MultiModalTrainingExample]) -> Dict[str, float]:
        """Evaluate the model on test data."""
        
        if not self.is_trained:
            raise ValueError("Model must be trained before evaluation")
        
        if not test_data:
            raise ValueError("Test data cannot be empty")
        
        # Prepare test data
        X_test, y_test = await self.model._prepare_training_data(test_data)
        
        # Make predictions
        predictions = await self.model._forward_pass(X_test, training=False)
        
        # Calculate metrics
        metrics = await self.model._calculate_all_metrics(predictions, y_test)
        
        logger.info(f"Evaluation metrics: {metrics}")
        
        return metrics
    
    async def save_model(self, filepath: Optional[str] = None) -> bool:
        """Save the trained model."""
        
        if filepath is None:
            filepath = f"{self.config.model_save_path}/model_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl"
        
        return await ModelPersistence.save_model(self.model, filepath)
    
    async def load_model(self, filepath: str) -> bool:
        """Load a trained model."""
        
        loaded_model = await ModelPersistence.load_model(filepath)
        
        if loaded_model:
            self.model = loaded_model
            self.recommendation_engine = RecommendationEngine(self.model)
            self.is_trained = loaded_model.is_trained
            return True
        
        return False
    
    def get_model_info(self) -> Dict[str, Any]:
        """Get information about the current model."""
        
        info = {
            "model_type": self.config.model_type.value,
            "training_mode": self.config.training_mode.value,
            "is_trained": self.is_trained,
            "architecture": {
                "hidden_layers": self.config.hidden_layers,
                "activation_function": self.config.activation_function,
                "dropout_rate": self.config.dropout_rate
            },
            "training_config": {
                "learning_rate": self.config.learning_rate,
                "batch_size": self.config.batch_size,
                "max_epochs": self.config.max_epochs
            }
        }
        
        if self.is_trained and self.model.training_history:
            info["training_history"] = {
                "epochs_trained": len(self.model.training_history["loss"]),
                "final_loss": self.model.training_history["loss"][-1] if self.model.training_history["loss"] else None,
                "final_accuracy": self.model.training_history["accuracy"][-1] if self.model.training_history["accuracy"] else None
            }
        
        return info